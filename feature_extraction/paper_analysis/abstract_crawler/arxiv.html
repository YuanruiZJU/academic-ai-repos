<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">

<head>
  <title>[1808.06601] Video-to-Video Synthesis</title>
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <!-- <link rel="stylesheet" type="text/css" media="screen" href="//static.arxiv.org/css/arXiv.css?v=20181218" /> -->
  <link rel="stylesheet" type="text/css" media="screen" href="/css/arXiv.css?v=20181218" />
  
  <!-- Piwik -->
  <script type="text/javascript">
    var _paq = _paq || [];
    _paq.push(["setDomains", ["*.arxiv.org"]]);
    _paq.push(['trackPageView']);
    _paq.push(['enableLinkTracking']);
    (function() {
      var u = "//webanalytics.library.cornell.edu/";
      _paq.push(['setTrackerUrl', u + 'piwik.php']);
      _paq.push(['setSiteId', 538]);
      var d = document,
        g = d.createElement('script'),
        s = d.getElementsByTagName('script')[0];
      g.type = 'text/javascript';
      g.async = true;
      g.defer = true;
      g.src = u + 'piwik.js';
      s.parentNode.insertBefore(g, s);
    })();
  </script>
  <!-- End Piwik Code -->
  <script type="text/javascript" src="https://culibrary.atlassian.net/s/d41d8cd98f00b204e9800998ecf8427e-T/-w6uc8u/b/16/a44af77267a987a660377e5c46e0fb64/_/download/batch/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector/com.atlassian.jira.collector.plugin.jira-issue-collector-plugin:issuecollector.js?locale=en-US&collectorId=7a8da419"></script>
<script type="text/javascript">window.ATL_JQ_PAGE_PROPS =  {
  "triggerFunction": function(showCollectorDialog) {
    //Requires that jQuery is available!
    jQuery("#feedback-button").click(function(e) {
      e.preventDefault();
      showCollectorDialog();
    });
  },
  fieldValues: {
    "components": ["15700"],  // Browse component.
    "versions": ["14132"],  // Release browse-0.1
    "customfield_11401": window.location.href
  }
  };
</script><link rel="stylesheet" media="screen" type="text/css" href="/bibex/bibex.css?20181010"/>
  <script src="//static.arxiv.org/js/mathjaxToggle.min.js" type="text/javascript"></script>
  <meta name="citation_title" content="Video-to-Video Synthesis"/>
  <meta name="citation_author" content="Wang, Ting-Chun"/>
  <meta name="citation_author" content="Liu, Ming-Yu"/>
  <meta name="citation_author" content="Zhu, Jun-Yan"/>
  <meta name="citation_author" content="Liu, Guilin"/>
  <meta name="citation_author" content="Tao, Andrew"/>
  <meta name="citation_author" content="Kautz, Jan"/>
  <meta name="citation_author" content="Catanzaro, Bryan"/>
  <meta name="citation_date" content="2018/08/20"/>
  <meta name="citation_online_date" content="2018/12/03"/>
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/1808.06601"/>
  <meta name="citation_arxiv_id" content="1808.06601"/><meta name="twitter:site" content="@arxiv"/>
    <meta property="twitter:title" content="Video-to-Video Synthesis"/>
    <meta property="twitter:description" content="We study the problem of video-to-video synthesis, whose goal is to learn a
mapping function from an input source video (e.g., a sequence of semantic
segmentation masks) to an output photorealistic..."/>
    <meta property="og:site_name" content="arXiv.org"/>
    <meta property="og:title" content="Video-to-Video Synthesis"/>
    <meta property="og:url" content="https://arxiv.org/abs/1808.06601v2"/>
    <meta property="og:description" content="We study the problem of video-to-video synthesis, whose goal is to learn a
mapping function from an input source video (e.g., a sequence of semantic
segmentation masks) to an output photorealistic video that precisely depicts
the content of the source video. While its image counterpart, the
image-to-image synthesis problem, is a popular topic, the video-to-video
synthesis problem is less explored in the literature. Without understanding
temporal dynamics, directly applying existing image synthesis approaches to an
input video often results in temporally incoherent videos of low visual
quality. In this paper, we propose a novel video-to-video synthesis approach
under the generative adversarial learning framework. Through carefully-designed
generator and discriminator architectures, coupled with a spatio-temporal
adversarial objective, we achieve high-resolution, photorealistic, temporally
coherent video results on a diverse set of input formats including segmentation
masks, sketches, and poses. Experiments on multiple benchmarks show the
advantage of our method compared to strong baselines. In particular, our model
is capable of synthesizing 2K resolution videos of street scenes up to 30
seconds long, which significantly advances the state-of-the-art of video
synthesis. Finally, we apply our approach to future video prediction,
outperforming several state-of-the-art competing systems."/>
</head>

<body  class="with-cu-identity">
  <noscript><img src="//webanalytics.library.cornell.edu/piwik.php?idsite=538&amp;rec=1" style="border:0;" alt="" /></noscript>
  <div id="cu-identity">
    <div id="cu-logo">
      <a href="https://www.cornell.edu/"><img src="//static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
    </div>
    <div id="support-ack">
      <a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br/>the Simons Foundation and member institutions.</a>
    </div>
  </div>

  <div id="header" >
    
  <h1><a href="/">arXiv.org</a> &gt; <a href="/list/cs/recent">cs</a> &gt; arXiv:1808.06601</h1>
  <div id="search">
    <form id="search-arxiv" method="get" action="https://arxiv.org/search">

      <div class="wrapper-search-arxiv">
        <input class="keyword-field" type="text" name="query" placeholder="Search or Article ID" />

        <div class="filter-field">
          <select name="searchtype">
            <option value="all">All fields</option>
            <option value="title">Title</option>
            <option value="author">Author(s)</option>
            <option value="abstract">Abstract</option>
            <option value="comments">Comments</option>
            <option value="journal_ref">Journal reference</option>
            <option value="acm_class">ACM classification</option>
            <option value="msc_class">MSC classification</option>
            <option value="report_num">Report number</option>
            <option value="paper_id">arXiv identifier</option>
            <option value="doi">DOI</option>
            <option value="orcid">ORCID</option>
            <option value="author_id">arXiv author ID</option>
            <option value="help">Help pages</option>
            <option value="full_text">Full text</option>
          </select>
        </div>
        <input class="btn-search-arxiv" value="" type="submit">
        <div class="links">(<a href="https://arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced search</a>)</div>
      </div>
    </form>
  </div>

  </div>

  <div id="content">
    <!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/1808.06601"
        dc:identifier="/abs/1808.06601"
        dc:title="Video-to-Video Synthesis"
        trackback:ping="/trackback/1808.06601" />
    </rdf:RDF>
-->
<div id="abs">
  <div class="extra-services">
    <div class="full-text">
      <span class="descriptor">Full-text links:</span>
      <h2>Download:</h2>
      <ul>
  <li><a href="/pdf/1808.06601" accesskey="f">PDF</a></li>
  <li><a href="/format/1808.06601">Other formats</a></li></ul>
      <div class="abs-license">(<a href="http://arxiv.org/licenses/nonexclusive-distrib/1.0/" title="Rights to this article">license</a>)</div>
    </div>
    <!--end full-text-->
    <div class="browse">
    <h3>Current browse context:</h3>
  <div class="current">cs.CV</div>

  <div class="prevnext">

  <span class="arrow">
    <a href="/prevnext?site=arxiv.org&amp;id=1808.06601&amp;function=prev&amp;context=cs.CV"
       accesskey="p" title="previous in cs.CV (accesskey p)">&lt;&nbsp;prev</a>
  </span>&nbsp;|&nbsp;

  
  <span class="arrow">
    <a href="/prevnext?site=arxiv.org&amp;id=1808.06601&amp;function=next&amp;context=cs.CV" accesskey="n"
       title="next in cs.CV (accesskey n)">next&nbsp;&gt;</a>
  </span><br/>
  </div><div class="list">
    <a href="/list/cs.CV/new">new</a>&nbsp;|
    <a href="/list/cs.CV/recent">recent</a>&nbsp;|
    <a href="/list/cs.CV/1808">1808</a>
  </div><h3>Change to browse by:</h3>
  <div class="switch">
    
      <a href="/abs/1808.06601?context=cs">cs</a>
      
    <br/>
    
      <span class="subclass"><a href="/abs/1808.06601?context=cs.GR">cs.GR</a></span>
      
    <br/>
    
      <span class="subclass"><a href="/abs/1808.06601?context=cs.LG">cs.LG</a></span>
      
    <br/>
    
  </div>
  
    </div>

    <div class="extra-ref-cite">
      <h3>References &amp; Citations</h3>
      <ul>
        
        <li><a href="https://ui.adsabs.harvard.edu/#abs/arXiv:1808.06601">NASA ADS</a></li>
      </ul>
    </div>

    <div class="dblp">
    <h3><a href="https://dblp.uni-trier.de">DBLP</a> - CS Bibliography</h3>
    <div class="list">
      <a href="https://dblp.uni-trier.de/db/journals/corr/corr1808.html#abs-1808-06601" title="listing on DBLP">listing</a> | <a href="https://dblp.uni-trier.de/rec/bibtex/journals/corr/abs-1808-06601" title="DBLP bibtex record">bibtex</a>
    </div>
    
    <div class="list">
      <a href="https://dblp.uni-trier.de/search/author?author=Ting-Chun%20Wang" title="DBLP author search">Ting-Chun Wang</a><br/><a href="https://dblp.uni-trier.de/search/author?author=Ming-Yu%20Liu" title="DBLP author search">Ming-Yu Liu</a><br/><a href="https://dblp.uni-trier.de/search/author?author=Jun-Yan%20Zhu" title="DBLP author search">Jun-Yan Zhu</a><br/><a href="https://dblp.uni-trier.de/search/author?author=Guilin%20Liu" title="DBLP author search">Guilin Liu</a><br/><a href="https://dblp.uni-trier.de/search/author?author=Andrew%20Tao" title="DBLP author search">Andrew Tao</a>
    
      <div class="list">&hellip;</div>
    
    </div>
    
  </div><div class="bookmarks">
  <div class="what-is-this"><h3>Bookmark</h3> (<a href="https://arxiv.org/help/social_bookmarking">what is this?</a>)</div><a href="/ct?url=http%3A%2F%2Fwww.citeulike.org%2Fposturl%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1808.06601&amp;v=e2581b63"
     title="Bookmark on CiteULike">
    <img src="//static.arxiv.org/icons/social/citeulike.png"
         alt="CiteULike logo" />
  </a>
  <a href="/ct?url=http%3A%2F%2Fwww.bibsonomy.org%2FBibtexHandler%3FrequTask%3Dupload%26url%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1808.06601%26description%3DVideo-to-Video+Synthesis&amp;v=024eecfc"
     title="Bookmark on BibSonomy">
    <img src="//static.arxiv.org/icons/social/bibsonomy.png"
         alt="BibSonomy logo"/>
  </a>
  <a href="/ct?url=https%3A%2F%2Fwww.mendeley.com%2Fimport%2F%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1808.06601&amp;v=5912fbf2"
     title="Bookmark on Mendeley">
    <img src="//static.arxiv.org/icons/social/mendeley.png"
         alt="Mendeley logo"/>
  </a>
  <a href="/ct?url=https%3A%2F%2Freddit.com%2Fsubmit%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1808.06601%26title%3DVideo-to-Video+Synthesis&amp;v=d3b1ab6c"
     title="Bookmark on Reddit">
    <img src="//static.arxiv.org/icons/social/reddit.png"
         alt="Reddit logo"/>
  </a>
  <a href="/ct?url=http%3A%2F%2Fsciencewise.info%2Fbookmarks%2Fadd%3Furl%3Dhttps%3A%2F%2Farxiv.org%2Fabs%2F1808.06601&amp;v=47deba23"
     title="Bookmark on ScienceWISE">
    <img src="//static.arxiv.org/icons/social/sciencewise.png"
         alt="ScienceWISE logo"/>
  </a>
</div>
  </div>
  <!--end extra-services-->

  <div class="leftcolumn">
    <div class="subheader">
      <h1>Computer Science > Computer Vision and Pattern Recognition</h1>
    </div>
    <h1 class="title mathjax"><span class="descriptor">Title:</span>Video-to-Video Synthesis</h1>
    <div class="authors"><span class="descriptor">Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+T">Ting-Chun Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu%2C+M">Ming-Yu Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu%2C+J">Jun-Yan Zhu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu%2C+G">Guilin Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tao%2C+A">Andrew Tao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kautz%2C+J">Jan Kautz</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Catanzaro%2C+B">Bryan Catanzaro</a>
    </div>

    <div class="dateline">(Submitted on 20 Aug 2018 (<a href="/abs/1808.06601v1">v1</a>), last revised 3 Dec 2018 (this version, v2))</div>

    <blockquote class="abstract mathjax"><span class="descriptor">Abstract:</span>  We study the problem of video-to-video synthesis, whose goal is to learn a
mapping function from an input source video (e.g., a sequence of semantic
segmentation masks) to an output photorealistic video that precisely depicts
the content of the source video. While its image counterpart, the
image-to-image synthesis problem, is a popular topic, the video-to-video
synthesis problem is less explored in the literature. Without understanding
temporal dynamics, directly applying existing image synthesis approaches to an
input video often results in temporally incoherent videos of low visual
quality. In this paper, we propose a novel video-to-video synthesis approach
under the generative adversarial learning framework. Through carefully-designed
generator and discriminator architectures, coupled with a spatio-temporal
adversarial objective, we achieve high-resolution, photorealistic, temporally
coherent video results on a diverse set of input formats including segmentation
masks, sketches, and poses. Experiments on multiple benchmarks show the
advantage of our method compared to strong baselines. In particular, our model
is capable of synthesizing 2K resolution videos of street scenes up to 30
seconds long, which significantly advances the state-of-the-art of video
synthesis. Finally, we apply our approach to future video prediction,
outperforming several state-of-the-art competing systems.
</blockquote>
    <!--CONTEXT-->
    <div class="metatable">
      <table summary="Additional metadata">
        <tr>
          <td class="tablecell label">Comments:</td>
          <td class="tablecell comments mathjax">In NeurIPS, 2018. Code, models, and more results are available at <a href="https://github.com/NVIDIA/vid2vid">this https URL</a></td>
        </tr>
        <tr>
          <td class="tablecell label">Subjects:</td>
          <td class="tablecell subjects"><span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Graphics (cs.GR); Machine Learning (cs.LG)</td>
        </tr><tr>
          <td class="tablecell label">Cite as:</td>
          <td class="tablecell arxivid"><a href="https://arxiv.org/abs/1808.06601">arXiv:1808.06601</a> [cs.CV]</td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxividv">(or <span class="arxivid">
              <a href="https://arxiv.org/abs/1808.06601v2">arXiv:1808.06601v2</a> [cs.CV]</span> for this version)
          </td>
        </tr>
      </table>
    </div>
    <div class="submission-history">
      <h2>Submission history</h2> From: Ting-Chun Wang [<a href="/show-email/4d587811/1808.06601">view email</a>]
      <br/>
  <b><a href="/abs/1808.06601v1">[v1]</a></b>
  Mon, 20 Aug 2018 17:58:42 UTC (2,695 KB)<br/><b>[v2]</b>
Mon, 3 Dec 2018 15:12:44 UTC (3,009 KB)<br/></div>
    <div class="endorsers"><a href="/auth/show-endorsers/1808.06601">Which authors of this paper are endorsers?</a> | <a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://arxiv.org/help/mathjax">What is MathJax?</a>)</div>
    <script type="text/javascript" language="javascript">mathjaxToggle();</script>
    <script src="/bibex/bibex.js?20181010" type="text/javascript" defer></script>
    
  </div>
  <!--end leftcolumn-->
</div>

  </div>
  <div id="footer">
    <div class="footer-text">
      <p>Link back to: <a href="https://arxiv.org/">arXiv</a>, <a href="/form">form interface</a>, <a href="https://arxiv.org/help/contact">contact</a>.&nbsp;&nbsp;<span class="help" style="display: inline-block; float: right"><a href="https://confluence.cornell.edu/x/MjmLFQ">Browse v0.1 released 2018-10-22</a>&nbsp;&nbsp;<button class="button is-small" id="feedback-button">Feedback?</button></span></p>
      <p class="a11y-text" style="margin: 1em 0; padding-top: 1em; border-top: 1px solid #ccc">If you have a disability and are having trouble accessing information on this website or need materials in an alternate format, contact <a href="mailto:web-accessibility@cornell.edu">web-accessibility@cornell.edu</a> for assistance.</p>
    </div>
    <div class="social"><a href="https://twitter.com/arxiv"><img src="//static.arxiv.org/icons/twitter_logo_blue.png" alt="Twitter" title="Follow arXiv on Twitter"/></a></div>
  </div>
</body>

</html>